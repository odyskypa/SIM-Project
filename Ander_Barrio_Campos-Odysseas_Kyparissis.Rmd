---
title: | 
    | Assignment 1 - Medical Cost
    | Statistical Inference and Modelling - SIM
    | 1st Semester 2022
author: "Ander Barrio Campos, Odysseas Kyparissis"
date: "`r Sys.Date()`"
geometry: margin=2cm
fontsize: 12pt
line-height: 1.5
output: 
  pdf_document:
    includes:
      in_header: header.tex
    toc: no
    number_sections: true
    fig_width: 6
    fig_height: 4
    fig_caption: true
classoption: a4paper
editor_options: 
  chunk_output_type: console
header-includes:
- \pagenumbering{gobble}
---

\newpage

```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
```{=tex}
\newpage
\pagenumbering{arabic}
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Clear plots from the R plots view:
if(!is.null(dev.list())) dev.off()

# Clean workspace - No variables at the current workspace
rm(list=ls())

# Setting working directory
setwd("C:/Users/odyky/Desktop/SIM-Project")

# Libraries loading
library(ggplot2)
library(GGally)
library(car)
library(arules)
library(lmtest)
library(chemometrics)
library(FactoMineR)
library(corrplot)
library(effects)
library(AER)
library(MASS)
```

\newpage

# Explanatory Data Analysis - EDA

## Loading Insurance Data

In this part of the report, setting up the working environment and loading of the data into R are taking place. Additionally, a first look at the summary of the raw insurance data set is taken.

```{r read}
df <- read.csv("insurance.csv")
summary(df)
```

## Data Types

To begin with, the types of the raw variables contained into the data set are being checked. It is clear, that the data set consists of 3 numerical variables and 4 categorical ones. The numeric variables are the following: age, BMI and charges, while the categorical ones are: sex, smoker, children and region. In the following sections, categorical variables will be transformed into labeled factors, as well as, new derived factors will be produced from the numerical values in order to see their performance on the regression modelling process.

```{r rawtypes, include=FALSE}
typeof(df$age)
typeof(df$sex)
typeof(df$bmi)
typeof(df$children)
typeof(df$smoker)
typeof(df$region)
typeof(df$charges)
```

## Checking for Missing Data

To continue with, a check for missing data is conducted on the raw data set. Considering the summary of the data set presented before, there are no NA values in the variables of the data set. The same conclusion is derived when a check is completed for each individual variable.

```{r missingData, include=FALSE}
ll <- which( df$age=="NA"); length(ll)
ll <- which( df$sex=="NA"); length(ll)
ll <- which( df$bmi=="NA"); length(ll)
ll <- which( df$children=="NA"); length(ll)
ll <- which( df$smoker=="NA"); length(ll)
ll <- which( df$region=="NA"); length(ll)
ll <- which( df$charges=="NA"); length(ll)
```

## Checking for Duplicates

By checking if there are duplicate rows inside the raw data set, the result indicates that a single occurrence of a duplicate exists and its index is equal to 582 as it is shown below.

```{r checkingDuplicates}
dupli <- duplicated(df); dupli_ind <- which(dupli); dupli_ind
```

With the following command, a closer look can be taken into the values of the duplicate row.

```{r presentDuplicate}
df [dupli_ind,]
```

To continue with the explanatory data analysis, the duplicate row is removed from the raw data set.

```{r removeDuplicate, include=FALSE}
df<- df[-dupli_ind,]
any(duplicated(df))
```

## Creating Factors for Qualitative Variables

In this subsection of EDA, all qualitative variables are transformed into labeled factors. The qualitative variables, as mentioned before, are sex, children, smoker and region. First of all, the unique values of these 4 variables are presented below:

```{r uniqueValeusOfQualitative}
unique(df$sex); unique(df$smoker); unique(df$region); unique(df$children)
```

The next step includes the creation of the labeled factors based on the unique values of the categorical variables. Following the practice below, in case a categorical variable includes NA values, they will be transformed into zeros, which is an incorrect approach. In this case, once missing values check indicated that there are no missing data, proceeding with this practice does not result in erroneous data.

```{r sexToFactor, echo=FALSE}
df$f.sex<-0; df$f.sex[df$sex=="male"]<-1
df$f.sex<-factor(df$f.sex, labels=c("F","M"))
```

```{r smokerToFactor, echo=FALSE}
df$f.smok<-0; df$f.smok[df$smoker=="yes"]<-1
df$f.smok<-factor(df$f.smok, labels=c("No","Yes"))
```

```{r regionToFactor, echo=FALSE}
df$f.reg<-0
df$f.reg[df$region=="southwest"]<-3
df$f.reg[df$region=="southeast"]<-2
df$f.reg[df$region=="northwest"]<-1
df$f.reg<-factor(df$f.reg,labels=c("NE","NW","SE","SW"))
```

```{r chidlrenToFactor, echo=FALSE}
df$f.children<-as.factor(df$children)
levels(df$f.children) <- c("0", "1", "2", "3", "4", "5")
```

## Creating Factors for Numerical Variables

This step is developed in order to extract factors from numerical variables. This approach's goal is to check if some variables are more descriptive as a factor rather than as a numeric feature while training linear models. From the numerical variables of the data set, only age and BMI will be converted to factors. Firstly, the discretization of the variable's values is taking place followed by the assigning of a label for each divided group.

### Age to Labeled Factor

```{r ageToFactor, echo=FALSE}
df$f.age<-df$age
df$f.age<-discretize(df$f.age, method = "interval", breaks = 3,
                 labels = c("Young","Medium", "Old"))
```

The result of the discretization for age is calculated by separating the values of the variable into 3 equally-interval groups with labels: "Young", "Medium" and "Old" respectively. The interval is equal to 15 years. Thus, group "Young" contain people in ages [18,33], group "Medium" contain individuals with ages [34,48] and finally "Old" group consist of people with ages [49,64].

### BMI to Labeled Factor

For BMI, the discretization of the numerical value will be completed by using the labels "Low", "Normal" and "High". The values for creating the groups in this step are selected based on the Adult Body Mass Index values from healthcare bibliography.

```{r bmiToFactor, echo=FALSE}
df$f.bmi<-df$bmi
df$f.bmi<-discretize(df$f.bmi, method = "fixed", breaks = c(-1,18.5,24.9,1000),
                 labels = c("Low","Normal", "High"))
```

## Factor Conversion Check

After checking both manually and by executing commands on the terminal, the conversion of the categorical and numerical variables to factors has been completed correctly. In addition, while the categorical variables sex, children, region and smoker have been transformed into labeled factors, their old versions are discarded from the data frame. Below is presented the new structure of the data frame.

```{r removingCHRVariables, echo=FALSE}
df$sex <- NULL #delete sex
df$region <- NULL #delete region
df$smoker <- NULL #delete smoker
df$children <- NULL #delete children
```

```{r dfStr}
str(df)
```

## Normal Distribution Test for Target Variable (charges)

By taking a look at the histogram of the target variable and the density curve that describe a normal distribution with mean and standard deviation equal to the respective values of the data set, one can understand that the target variable does not follow a normal distribution. In order to be precise, by running the Shapiro test, the result indicate a value less than 0.05. Thus, the null hypothesis can be rejected and conclude that the target variable does not follow a normal distribution.

```{r nomrlaDistHist, echo=FALSE}
hist(df$charges,freq=F,breaks=10)
m=mean(df$charges);std=sd(df$charges)
curve(dnorm(x,m,std),col="red",lwd=2,add=T)
```

```{r shapiroTest}
shapiro.test(df$charges)
```

## Serial Correlation

In order to address the serial correlation for the target variable two different approaches were followed. Firstly, the autocorrelation function was used which produces the ACF graph shown below.

```{r acf, echo=FALSE}
acf(df$charges)
```

From the graph, one can understand that all the vertical lines are inside the two horizontal blue lines except for the first one. The interpretation of this result is that there is no serial correlation for the target variable. Furthermore, for using statistical methods to address the same problem, the Durbin-Watson (DW) test was applied. The result is presented here:

```{r dwTest}
dwtest(df$charges~1)
```

Once the resulting p-value is equal to 0.5 approximately, it means that the null hypothesis can not be rejected. The Durbin-Watson test has the null hypothesis that the autocorrelation of the disturbances is 0, thus serial correlation for the target variable is discarded.

## Outliers Detection

```{r calcQFunction, echo=FALSE}
calcQ <- function(x) {
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3], 
       q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) }
```

In the following subsections both uni-variate and multivariate outliers will be detected and treated.

### Univariate Outliers

To start with, in the following subsection the uni-variate outliers will be detected for the numerical variables: age, bmi and charges with the respective order. It is crucial to mention here, that only severe outliers were taken into account and not mild ones. Now, concerning variable age, as it is depicted in the boxplot of the variable, outliers do not exist. The same result is derived after trying to detect outliers using the IQR method, which is implemented by function calcQ.

```{r uniOutAgeBoxPlot, message=FALSE, echo=FALSE, fig.height=3, fig.width=6}
boxplot(df$age, main = "Boxplot of Variable Age")
```

```{r uniOutAgeIQR, message=FALSE, echo=FALSE}
var_out<-calcQ(df$age)
llout_age<-which((df$age<var_out$souti)|(df$age>var_out$souts))
#length(llout_age)
```

Following by, the same approach is used for variable BMI.

```{r uniOutBMIBoxPlot, message=FALSE, echo=FALSE,fig.height=3, fig.width=6}
boxplot(df$bmi, main = "Boxplot of Variable BMI")
var_out<-calcQ(df$bmi)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")
abline(h=var_out$mouts,col="red")
llout_bmi<-which((df$bmi<var_out$souti)|(df$bmi>var_out$souts))
#length(llout_bmi)
```

The results are the same, there are no severe outliers for variable BMI as well, but in this case some mild ones appear but will not be treated. Finally, the outlier detection for the target variable is taking place.

```{r uniOutCharges, echo=FALSE, fig.height=3, fig.width=6}
#Boxplot(df$charges, main = "Boxplot of Variable Charges") #Using boxplot
# in order to hide the outlier indexes in the report for space saving
boxplot(df$charges, main = "Boxplot of Variable Charges")
var_out<-calcQ(df$charges)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")
abline(h=var_out$mouts,col="red")
llout_charges<-which((df$charges<var_out$souti)|(df$charges>var_out$souts))
length(llout_charges)
```

In this case, there are 6 extreme outliers for the target variable, which are presented below.

```{r chargesOutliers}
df[llout_charges,]
```

Due to the fact that the outliers are presented for the target variable, treating them would not be an ideal approach. By checking the values of the data set for those 6 cases, one conclusion that can be drawn is that all of them describe people who smoke and have a high BMI. The values of the other variables for those 6 cases are fluctuating. For this reason, it was decided to remove those observations from our further analysis.

```{r removingChargesOutliers}
df <- df[-llout_charges,]
```

### Multivariate Outliers

In this subsection, the detection of multivariate outliers is taking place. As a first step, the calculation of the Mahalanobis distance with a confidence interval of 95% is completed. The Classical and Robust Mahalanobis distances for the numerical values of the dataset are presented in the following figure:

```{r mOutMaha, echo=FALSE}
res.mout <- Moutlier( df[ , c(1:3)], quantile = 0.95 )
```

After calculating Mahalanobis distance at a 95% confidence interval, the cut off given is 2.795483.

```{r mOutCutOff, include=FALSE}
res.mout$cutoff
```

Then, all the observations which have a classical and a robust distance bigger than this cut off are marked as multivariate outliers. After detecting them, a new factor ("mout") is being created in the data set, indicating if an observation belongs to multivariate outliers or not. It can be seen in the final result that 63 observations are marked as multivariate outliers. Further analysis about them will be conducted in the following sections.

```{r mOutPlot, echo=FALSE}
par(mfrow=c(1,1))
plot( res.mout$md, res.mout$rd )
text(res.mout$md, res.mout$rd, labels=rownames(df),adj=1, cex=0.5)
abline( h=res.mout$cutoff, lwd=2, col="red")
abline( v=res.mout$cutoff, lwd=2, col="red")
```

```{r detectMout, echo=FALSE}
llmout <- which((res.mout$md > res.mout$cutoff) 
                 & (res.mout$rd > res.mout$cutoff))
df$mout <- 0
df$mout[llmout] <- 1
df$mout <- factor( df$mout, labels = c("MvOut.No","MvOut.Yes"))
```

```{r summaryMout, echo=FALSE}
summary(df["mout"])
```

## Preliminary Exploratory Analysis

The goal of this chapter is to discover the relationships between the different variables of the data set and the target variable. In order to do so, the following techniques were used: calculation of the Spearman correlation for the numerical variables, calculating and presenting interactions between the target and explanatory variables by using the library FactoMineR and Boxplots.

In the following graph, the correlation of the numerical values is presented. The only significant observation here is that age and charges have a slightly strong correlation. By checking the correlation matrix the value between those two variables is equal to 0.53.

```{r corrPlot, echo=FALSE}
M <- cor(df[,c(3,1:2)],method="spearman");M #Non Parametric version
corrplot(M, method="circle")
```

Moreover, with the usage of the library FactoMineR and specifically the function condes, which calculate the dependencies of a continuous variable, it is able to check the dependencies of the target variable with the rest of the variables of the data set. At first, the dependency between the target variable and the numerical variables will be skipped as we have already calculated the correlation before.

```{r condesQuanti, include=FALSE}
res.con<-condes(df[,1:9],3)
res.con$quanti
```

Furthermore, the relationship of the target variable with the qualitative variables is conducted below.

```{r condesQuali, echo=FALSE}
res.con$quali
```

It is obvious, that the most important relationship (for the categorical variables) exist between charges and the factor "f.smok" with an R-square value approximately equal to 0.62 and an extremely low p-value. The rest of the categorical variables ("f.age", "f.bmi", "f.sex", "f.children" and "f.reg") do not show a great relationship with the target variable since the R-squared values they achieve is really small (close to 0).

A closer look to the relationship of charges and the different categories of the categorical variables is presented below.

```{r condesCategory, echo=FALSE}
res.con$category
```

By interpreting the results, one can understand that the most influential categories that lead to the maximization of the values of the target variable is people who are smokers, following by old people and people with high BMI values or people that have 2 or 3 children as dependents. On the other hand, exactly the opposite is true for non smokers and young people. A visual interpretation of the above conclusions with the usage of Boxplots exist in the Appendix.

Additionally, association tests on mean and variance are completed between the target variable and the different explanatory variables, based on their type. It is important to mention here, that only non-parametric tests were completed due to the fact that charges do not follow a normal distribution. Firstly, Kruskal test is used for checking the mean of the target variable between the different groups of region, children, age and BMI multi-level factors.

Kruskal mean test for charges, based on the different groups of age, showed that the p-value is extremely small, so null hypothesis can be rejected. Thus, it can be said that the different groups of age have different mean of the variable charges, same is true for the 5 groups of "f.children" factor. With the same approach, the same result is derived for categorical variables "f.reg" and "f.bmi", but here the p-values are not very small, in fact they are bigger than 0.01. This means that the difference of the means in the specific sample is not so significant.

To continue with, in order to test the mean difference between the different groups of binary factors the usage of Wilcox test is necessary. The test is completed once for the variable "f.sex" and once for "f.smoke". It is obvious, that for variable "f.smok" the p-value is extremely low (rejecting null hypothesis), thus the mean of the target variable for smokers and non-smokers have a significant difference, while for variable "f.sex" the p-value is large (0.69), meaning that mean values of charges do not have significant differences for men and women.

Moving on, the variance tests are taking place. For the same reason as for the mean tests, only non-parametric tests were used. In the case of variance, Flinger test covers both multi-leveled factors and binary ones.

While Fligner test has as a null hypothesis the fact that variances in each of the groups defined by the levels of the factors are the same. With this analysis the following results are concluded. For variable "f.age" we have a large p-value (0.89) indicating that the variance of charges in the different age groups is the same. For variable "f.reg" the p-value is less than 0.05 (0.0003) so here one can say that the variance of charges in different regions have a significant difference. The same is true for variable "f.sex" and "f.children" which achieves a p-value equal to 0.001. To continue with, for variable "f.bmi" the p-value of the test is slightly smaller than 0.05 (0.042), which means that the null hypothesis is rejected, but due to the fact that the p-value is close to 0.05, it is concluded that the difference on the variance in the different BMI groups is not so significant. On the other hand variable "f.smok" achieves a p-value close to 0 (2.2e-16) showing that the difference of variance for variable charges is totally significant between smokers and non-smokers.

All the results mentioned in this section can be found in the Appendix as well, in the respective sub-sections.

# Linear Regression Modelling

In this section of the report, the creation and comparison of multiple linear regression models is completed. The approach followed is presented here. As a first step, a linear model is trained addressing the target variable by using only numerical variables. Then, the addition of factors in the model is taking place, as well as the interactions between the different variables of the data set. Between each step, the quality of the models is being tested. The quality assurance of the models include the interpretation of several plots, like residuals plots, influential data plots, all-effects plots, etc.. Additionally, filtering of the influential data and several transformations on the dataset's variables are taking place in order to correct the final prediction quality of the model. The whole procedure is described in details in the next subsections. For space saving reasons again, most of the plots are presented in the Appendix, at the appropriate subsections.

## Addressing Target Variable only with Numerical Variables

In this subsection, the training of a linear model by using only the numerical variables of the data set is achieved. The numerical explanatory variables used are age and BMI.

```{r lm0}
m0 <- lm(charges~age+bmi, data=df)
```

By checking the results of summary and vif for model0 (shown in Appendix), all coefficients for the two numerical variables have small p-values, meaning that the null hypothesis (coefficient equal to zero) can be rejected. The same is true for the intercept. Also, the difference in the significance of the p-values can be seen, indicating that variable age is the most important following by BMI. Additionally, R-squared value of the model is very small (0.1151) suggesting that the linear model is poor. Finally, the Variance Inflation Factor (VIF) quantifies the extent of correlation between the predictors of a model, and since the resulting values are close to 1 it means that there is no multi-collinearity. The same interpretation will be followed for every model created below, plus some extra plots for residuals quality checking and influential data and residual outliers, - only the important ones are presented - all other information for all models can be found in the Appendix.

```{r lm0Plot, echo=FALSE}
par(mfrow=c(2,2))
plot(m0, main = "Model m0")
par(mfrow=c(1,1))
```

By checking the fitted vs residuals plot, one can understand that heteroscedasticity is present in the regression model. Also by checking the Normal Q-Q plot, the residuals do not follow a normal distribution. Thus, the quality of the model for predicting the target variable is poor and needs to be upgraded.

### Transformation of Target and Explanatory Variables

The next step, with goal the improvement of the model, is to check if any transformation, both in the target or the explanatory variables, will improve the quality of the model. Firstly, a transformation on the target variable is tested. In order to check that, boxcox transformation is used.

```{r transformations, message=FALSE, fig.height=3, fig.width=6}
boxcox(charges~age+bmi, data=df)
boxTidwell(log(charges)~age+bmi, data = df)
```

From the previous plot, it is obvious that the result of the boxcox transformation is very close to zero, assuming that a logarithmic transformation is needed for the target variable. Moreover, from the boxTidwell test results, a transformation for variable age is suggested due to the significance of the test. Once the MLE of lambda for age is equal to -0.012223 which is approximately 0, logarithmic transformation was used as well.

```{r lm1}
m1 <- lm(log(charges)~log(age)+bmi, data=df)
```

After applying the logarithmic transformation in the target variable (charges) and the explanatory variable age, an improvement is show with respect the R-squared value of the model, 0.2916 compared to 0.1189 of model m0. On the other hand by taking a look at the residual plots, it is noticed from the first graph that more heteroscedasticity is present now, but the residuals start to become more normal than before. The cook distance from the fourth graph seams to be similar (graphs at Appendix).

### Residual and Influential Data for Numerical Model

The next step conducted during the modelling procedure, is to check the existence of remarkable residual outliers and influential data. The residual and marginal model plots as well as Breusch-Pagan test were used, showing that, transformation of age, bmi and the fitted values satisfy the linearity condition. Also, from the marginal model plot it is clear that the regressors do not need any more transformations. Finally, heteroscedasticity is presented in the model and with big significance (p-value very small for bptest).

```{r residualPlotM1, echo=FALSE}
residualPlots(m1, main = "Model m1")
par(mfrow=c(1,1))
```

In addition, from the added-variable and influence plots, one can understand that potentially influential observations can be found for both log(age) and BMI variables.

```{r influencePlotM1, echo=FALSE}
influencePlot(m1,main = "Model m1")
```

With the usage of cooks distance and boxplots, residual outliers can be detected. This approach has been used here and the following observations are characterized as residual outliers.

```{r residualOutM1, echo=FALSE}
llcoo<-Boxplot(cooks.distance(m1), id=list(n=2,labels=row.names(df)))
df[llcoo,]
```

```{r removingResidualOutM1, echo=FALSE}
resOut_m1 <- df[llcoo,]
ll<-which(row.names(df) %in% c("1048", "1157"))
df_1 <- df[-ll,]
```

One thing that is noticeable here is that residual outliers are multivariate outliers as well. The same procedure is followed here for the influential observations by detecting the a-priori influential observations with the help of the hat matrix and the hat-values located in its diagonal. This procedure has been repeated 3 times before moving on to the insertion of factors to the linear model. A-priori influential data and residual outliers are presented in each of the 3 iterations, and then they are filtered out of the data frame.

```{r influnceObsM1, echo=FALSE}
llhat<-which(hatvalues(m1)>3*length(coef(m1))/nrow(df_1))
df_1[llhat,]
inflOut_m1 <- df_1[llhat,]
df_2 <- df_1[-llhat,]
```

```{r retrainM1intoM2}
m2 <- lm(log(charges)~log(age)+bmi, data=df_2)
```

```{r residualOutM2, echo=FALSE}
llcoo<-Boxplot(cooks.distance(m2), id=list(n=2,labels=row.names(df_2)))
df_2[llcoo,]
```

```{r removingResidualOutM2, echo=FALSE}
resOut_m2 <- df_2[llcoo,]
ll<-which(row.names(df_2) %in% c("804", "1318"))
df_3 <- df_2[-ll,]
```

```{r influnceObsM2, echo=FALSE}
llhat<-which(hatvalues(m2)>3*length(coef(m2))/nrow(df_3))
df_3[llhat,]
inflOut_m2 <- df_3[llhat,]
df_4 <- df_3[-llhat,]
```

```{r retrainM2intoM3}
m3 <- lm(log(charges)~log(age)+bmi, data=df_4)
```

```{r residualOutM3, echo=FALSE}
llcoo<-Boxplot(cooks.distance(m3), id=list(n=2,labels=row.names(df_4)))
df_4[llcoo,]
```

```{r removingResidualOutM3, echo=FALSE}
resOut_m3 <- df_4[llcoo,]
ll<-which(row.names(df_4) %in% c("760", "293"))
df_5 <- df_4[-ll,]
```

```{r influnceObsM3, echo=FALSE}
llhat<-which(hatvalues(m3)>3*length(coef(m3))/nrow(df_5))
df_5[llhat,]
inflOut_m3 <- df_5[llhat,]
df_6 <- df_5[-llhat,]
```

```{r retrainM3intoM4}
m4 <- lm(log(charges)~log(age)+bmi, data=df_6)
```

One important fact about the infuential observations and residual outliers in all of the 3 filtering steps conducted above is that they all belong to observations with high BMI values. After completing the filtering of residual outliers and influential data from the quality graphs for model m4 (Appendix), it is obvious that heteroscedasticity still remains, but the residuals are more close to following a normal distribution and cook's distance has been reduced significantly. Also, the numerical variables provide a better fit to the model as shown in the marginal model plot.

## Addition of Categorical Effects into the Model

The goal of this subsection is to add the important effects and interactions of the categorical variables into the linear model. To begin with, all the effects of the categorical variables are added to the model and then its quality is being tested. Function Anova is used in order to conduct the Net effects test.

```{r lm5}
m5 <- lm(log(charges)~log(age) + bmi + f.sex + f.smok + f.reg
         + f.children, data=df_6)
```

From the results presented in Appendix (Quality Info Model m5), summary shows that all the variables added are providing explanatory power to the model, and they all have very small p-values during the Anova test, except for sex. Moreover, with the use of VIF, it is derived that multi-collinearity does not exist, since all values are smaller than 3. Due to the result of the function anova it is decided that factor sex will be removed from the model.

```{r lm6}
m6 <- lm(log(charges)~log(age) + bmi + f.smok + f.reg
         + f.children, data=df_6)
```

After completing the quality check of model m6, from the summary it is clear that every coefficient is different from zero with great statistical significance. The same is derived from the results of the anova test. In addition, vif results are lower than 3 again, so multi-collinearity does not exist. From allEffects plot for model m6, it is clear that for factor "f.reg" there is an overlap for categories "SE" and "SW". The same can be seen for variable "f.children" for the combination of categories "2" and "3", and "4" and "5". Also for variable BMI, its clear that the model does have a good fit to the target variable. For that reason the creation of new factors for those variables is completed below. Heteroscedasticity still remains as it is shown in the residual plots, and the residuals do not still follow a normal distribution. Additionally, the existance of some residual outliers can be seen in the residuals vs leverage plot.

```{r newChildrenFactor}
df_6$f.Children_groups<-0
ll<-which(df_6$f.children %in% c("1"))
df_6$f.Children_groups[ll]<-1
ll<-which(df_6$f.children %in% c("2","3"))
df_6$f.Children_groups[ll]<-2
ll<-which(df_6$f.children %in% c("4","5"))
df_6$f.Children_groups[ll]<-3
df_6$f.Children_groups <- factor( df_6$f.Children_groups,
                                labels=c("f.0_Children",
                                         "f.1_Child",
                                         "f.2-3_Children",
                                         "f.4-5_Children"))
```

```{r factorSouth}
df_6$f.South<-0
ll<-which(df_6$f.reg %in% c("SE","SW"))
df_6$f.South[ll]<-1
df_6$f.South <- factor( df_6$f.South, labels=c("f.North","f.South"))
```

```{r factorBMI_H}
df_6$f.BMI_H<-0
ll<-which(df_6$f.bmi %in% c("High"))
df_6$f.BMI_H[ll]<-1
df_6$f.BMI_H <- factor( df_6$f.BMI_H, labels=c("f.BMI-Low-Normal","f.BMI-High"))
```

```{r lm7}
m7 <- lm(log(charges)~log(age) + f.BMI_H + f.smok + f.South
         + f.Children_groups, data=df_6)
```

```{r allEffectsM7}
plot(allEffects(m7), main = "Model m7")
```

After changing the factors of region and children, with the allEffects plot one can understand that model m7 provides a better discrimination of the target variable with the new categories. All other quality tests and metrics are similar to m6. Next step followed for the upgrade of model is the addition of interactions between the new categorical variables and the logarithmic representation of variable age.

## Addition of Interactions into the Model

In this subsection, interactions between log(age) and all the factors are included in the model. Then with the help of function step, all insignificant interactions are being removed from the model. Finally, influential data and residual outliers are being spotted and treated and consequently the final model is being trained with the latest version of the data frame. To conclude all plots and tests for the final model are described in detail.

```{r lm8}
m_all <- lm(log(charges)~log(age)*(f.BMI_H + f.smok + f.South
         + f.Children_groups), data=df_6)
m8 <- step(m_all)
```

From the results of the step function, one can understand that the new model contains as explanatory variables the logarithmic transformation of age, the effects of factors: "f.BMI_H", "f.smok", "f.South", "f.Children_groups", and the interactions: "log(age):f.smok", "log(age):f.South" and finally "log(age):f.Children_groups".

```{r qualitym8a}
summary(m8)
anova(m8)
vif(m8,type="predictor")
```

From the VIF results, it is clear that now multi-collinearity exists for the interaction of log(age) with the factors "f.smok" and "f.South". For that reason those interactions are being removed from the model. Thus, the only interaction left in the model is between log(age) and the factor "f.Children_groups".

```{r lm9}
m9 <- lm(log(charges) ~ log(age) + f.BMI_H + f.smok + f.South + 
    f.Children_groups + log(age):f.Children_groups, data = df_6)
```

Finally, after running the command summary for model m9, it is obvious that the R-squared value has been increased significantly (0.7675). Additionally, anova test shows that all variables used in the modelling are significant. To continue with, after removing the redundant interactions of factor "f.Children_groups" the VIF results showing that there is not multi-collinearity any more. From the residuals plots, similar conclusions are derived as before. Heteroskedasticity exists in the model, (bptest is being rejected as well), the residuals fail to follow a normal distribution, and there are some influetial data in the model. By the allEffects plot and the summary of the model it is clear that there is overlap for the categories 0_Children and 1_Child, as well as, for the categories "4-5_Children" and "2-3Children". For that reason re-factoring of this factor is conducted below.

```{r lastChildrenFactor}
df_6$f.final_Children<-0
ll<-which(df_6$f.children %in% c("0","1"))
df_6$f.final_Children[ll]<-0
ll<-which(df_6$f.children %in% c("2","3","4","5"))
df_6$f.final_Children[ll]<-1
df_6$f.final_Children <- factor( df_6$f.final_Children,
                                labels=c("f.0-1_Childn",
                                         "f.2-5_Children"))
```

```{r lm10}
m_all_new <- lm(log(charges) ~ log(age) * (f.BMI_H + f.smok + f.South + 
    f.final_Children), data = df_6)
m10 <-step(m_all_new)
```

```{r lm11}
m11 <- lm(log(charges) ~ log(age) + f.BMI_H + f.smok + f.South + 
    f.final_Children + log(age):f.final_Children,data = df_6)

```

Finally after all those changes we can see that multi-colinearity is added in the model, and the resulting model is not better than the one we had at m9. Thus, the final model which will be used is m9. In the following subsection, the analysis or residual outliers and influential data for model m9 and the data frame used to train it will take place.

## Final Model - Removal of Residual Outliers and Influetial Data

The first step for reaching the final version of model m9 is to remove the residual outliers and the influential data. This procedure is completed below:

```{r residualOutM9, echo=FALSE}
llcoo<-Boxplot(cooks.distance(m9), id=list(n=2,labels=row.names(df_6)))
df_6[llcoo,]
```

```{r removingResidualOutM9, echo=FALSE}
resOut_m9 <- df_6[llcoo,]
ll<-which(row.names(df_6) %in% c("1013", "322"))
df_7 <- df_6[-ll,]
```

```{r influnceObsM9, echo=FALSE}
llhat<-which(hatvalues(m9)>3*length(coef(m9))/nrow(df_7))
df_7[llhat,]
inflOut_m9 <- df_7[llhat,]
df_8 <- df_7[-llhat,]
```

Again here it is concluded that most of the influential data and residual outliers come from people with very high BMI value, but the values of those observations for the remaining variables are fluctuating a lot.

```{r retrainM9intoM12}
m12 <- lm(log(charges) ~ log(age) + f.BMI_H + f.smok + f.South + 
    f.Children_groups + log(age):f.Children_groups, data = df_8)
```

```{r qualitym12}
summary(m12)
anova(m12)
vif(m12,type="predictor")
```

From the summary, and anova functions of the final model, one can conclude that all variables in the linear model provide explanatory power. The only interaction which is not so powerful is between the logarithmic transformation of age and the category "1_Child" of factor "f.Children_groups". Moreover from the results of VIF, it is clear that multi-colinearity does not exist.

```{r, echo=FALSE}
plot(allEffects(m12), main = "Model m12")
```

AllEffects plot shows that factors "f.BMI_H", "f.smok" and "f.South" are separated correctly and they explain well the variance of the target variable. The same problem exists with the categories of factor "f.Children_groups".

```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(m12, id.n=0, main = "Model m12")
par(mfrow=c(1,1))
bptest(m12)
```

Additionally, from the residuals plot, again it is concluded that, heteroskedasticity exist in the model. Several different approaches were followed for reducing heteroskedasticity during this assignment, but it was impossible to achieve that. The same is concluded from the bptest completed above (p-value \<0.05). The residuals normality test is failing always, showing a large discrepancy in the top right corner of the Normal Q-Q plot of the model. Concerning the residual outliers on the residuals vs leverage plot, only 2 possible points seem like outliers, but after manually removing them from the data set, nothing is changing significantly. Those points were located from the following plot

```{r, echo=FALSE}
influencePlot(m12,main = "Model m12")
```

Finally, the model does not fit very well the data. It was anticipated to see the residuals following a normal distribution after the removal of outliers and influential data , but it did not happen. For that reason the quality plots show that some of the multiple linear modelling assumptions are not followed. The conclusion is that a linear regression is not good enough to predict the charges based on the explanatory variables and maybe a more complex predictive model could help.

```{r, echo=FALSE, message=FALSE}
#marginalModelPlots(m12, main = "Model m12")
#avPlots(m12, id=list(method=list(cooks.distance(m12),"x"), n = 5),
#        main = "Model m12")

```

To sum up, the final model predict the logarithmic transformation of charges using as explanatory variables the logarithmic transformation of age, the main effects of the factors "f.BMI_H", "f.smok", "f.South", "f.Children_groups" and finally the interaction between log(age) and factor "f.Children_groups" (log(age):f.Children_groups).

All different models, their quality plots and tests, and some extra tries for improving the quality of the linear model exist in the Appendix.

\newpage

# Appendix

## EDA

### Visual Interpretation of Associations with Boxplots

```{r visualInteractionsSEX, echo=FALSE}
#SEX
par(mfrow=c(1,1))
boxplot(charges~f.sex,data=df, id=list(n=Inf,labels=row.names(df)),
        main = "Association of Charges and Sex")
```

```{r visualInteractionsREGION, echo=FALSE}
#REGION
par(mfrow=c(1,1))
boxplot(charges~f.reg,data=df, id=list(n=Inf,labels=row.names(df)),
        main = "Association of Charges and Region")
```

```{r visualInteractionsSMOKER, echo=FALSE}
#SMOKER
par(mfrow=c(1,1))
boxplot(charges~f.smok,data=df, id=list(n=Inf,labels=row.names(df)),
        main = "Association of Charges and Smokers")
```

```{r visualInteractionsAGE, echo=FALSE}
#AGE
par(mfrow=c(1,1))
boxplot(charges~f.age,data=df, id=list(n=Inf,labels=row.names(df)),
        main = "Association of Charges and Age")
```

```{r visualInteractionsBMI, echo=FALSE}
#BMI
par(mfrow=c(1,1))
boxplot(charges~f.bmi,data=df, id=list(n=Inf,labels=row.names(df)),
        main = "Association of Charges and BMI")
```

### Kruskal and Wilcox Tests for Mean

```{r testOnMeansMultiFactors}
kruskal.test(charges~f.age,data=df)
kruskal.test(charges~f.reg,data=df)
kruskal.test(charges~f.bmi,data=df)
kruskal.test(charges~f.children,data=df)
```

```{r testOnMeansBinaryFactors}
wilcox.test(charges~f.sex,data=df, correct = TRUE, exact= FALSE)
wilcox.test(charges~f.smok,data=df, correct = TRUE, exact= FALSE)
```

### Flinger Tests for Variance

```{r testOnVarAllFactors}
fligner.test(charges~f.age,data=df) # Non Parametric
fligner.test(charges~f.reg,data=df)
fligner.test(charges~f.bmi,data=df)
fligner.test(charges~f.sex,data=df)
fligner.test(charges~f.smok,data=df)
fligner.test(charges~f.children,data=df)
```

## Modelling

### Quality Info Model m0

```{r qualityM0}
summary(m0)
vif(m0)
```

### Quality Info Model m1

```{r qualityM1}
summary(m1)
vif(m1)
par(mfrow=c(2,2))
plot(m1, main = "Model m1")
par(mfrow=c(1,1))
marginalModelPlots(m1, main = "Model m1")
bptest(m1)
avPlots(m1, id=list(method=list(cooks.distance(m1),"x"), n = 5),
        main = "Model m1")
```

### Quality Info Model m2

```{r qualityM2}
summary(m2)
par(mfrow=c(2,2))
plot(m2, id.n=0, main = "Model m2")
par(mfrow=c(1,1))
marginalModelPlots(m2, main = "Model m2")
bptest(m2)
avPlots(m2, id=list(method=list(cooks.distance(m2),"x"), n = 5),
        main = "Model m2")
residualPlots(m2, main = "Model m2")
par(mfrow=c(1,1))
influencePlot(m2,main = "Model m2")
```

### Quality Info Model m3

```{r qualityM3}
summary(m3)
par(mfrow=c(2,2))
plot(m3, id.n=0, main = "Model m3")
par(mfrow=c(1,1))
marginalModelPlots(m3, main = "Model m3")
bptest(m3)
avPlots(m3, id=list(method=list(cooks.distance(m3),"x"), n = 5),
        main = "Model m3")
residualPlots(m3, main = "Model m3")
par(mfrow=c(1,1))
influencePlot(m3,main = "Model m3")
```

### Quality Info Model m4

```{r qualityM4}
summary(m4)
par(mfrow=c(2,2))
plot(m4, id.n=0, main = "Model m4")
par(mfrow=c(1,1))
marginalModelPlots(m4, main = "Model m4")
bptest(m4)
avPlots(m4, id=list(method=list(cooks.distance(m4),"x"), n = 5),
        main = "Model m4")
residualPlots(m4, main = "Model m4")
par(mfrow=c(1,1))
influencePlot(m4,main = "Model m4")
```

### Quality Info Model m5

```{r qualityM5}
summary(m5)
anova(m5)
vif(m5,type="predictor")
plot(allEffects(m5), main = "Model m5")
par(mfrow=c(2,2))
plot(m5, id.n=0, main = "Model m5")
par(mfrow=c(1,1))
marginalModelPlots(m5, main = "Model m5")
bptest(m5)
avPlots(m5, id=list(method=list(cooks.distance(m5),"x"), n = 5),
        main = "Model m5")
residualPlots(m5, main = "Model m5")
par(mfrow=c(1,1))
influencePlot(m5,main = "Model m5")
```

### Quality Info Model m6

```{r qualitym6}
summary(m6)
anova(m6)
vif(m6,type="predictor")
plot(allEffects(m6), main = "Model m6")
par(mfrow=c(2,2))
plot(m6, id.n=0, main = "Model m6")
par(mfrow=c(1,1))
marginalModelPlots(m6, main = "Model m6")
bptest(m6)
avPlots(m6, id=list(method=list(cooks.distance(m6),"x"), n = 5),
        main = "Model m6")
residualPlots(m6, main = "Model m6")
par(mfrow=c(1,1))
influencePlot(m6,main = "Model m6")
```

### Quality Info Model m7

```{r qualitym7}
summary(m7)
anova(m7)
vif(m7,type="predictor")
par(mfrow=c(2,2))
plot(m7, id.n=0, main = "Model m7")
par(mfrow=c(1,1))
marginalModelPlots(m7, main = "Model m7")
bptest(m7)
avPlots(m7, id=list(method=list(cooks.distance(m7),"x"), n = 5),
        main = "Model m7")
residualPlots(m7, main = "Model m7")
par(mfrow=c(1,1))
influencePlot(m7,main = "Model m7")
```

### Quality Info Model m8

```{r qualitym8}
summary(m8)
anova(m8)
plot(allEffects(m8), main = "Model m8")
par(mfrow=c(2,2))
plot(m8, id.n=0, main = "Model m8")
par(mfrow=c(1,1))
marginalModelPlots(m8, main = "Model m8")
bptest(m8)
avPlots(m8, id=list(method=list(cooks.distance(m8),"x"), n = 5),
        main = "Model m8")
residualPlots(m8, main = "Model m8")
par(mfrow=c(1,1))
influencePlot(m8,main = "Model m8")
```

### Quality Info Model m9

```{r qualitym9}
summary(m9)
anova(m9)
vif(m9,type="predictor")
plot(allEffects(m9), main = "Model m9")
par(mfrow=c(2,2))
plot(m9, id.n=0, main = "Model m9")
par(mfrow=c(1,1))
marginalModelPlots(m9, main = "Model m9")
bptest(m9)
avPlots(m9, id=list(method=list(cooks.distance(m9),"x"), n = 5),
        main = "Model m9")
residualPlots(m9, main = "Model m9")
par(mfrow=c(1,1))
influencePlot(m9,main = "Model m9")

```

### Quality Info Model m10

```{r qualitym10}
summary(m10)
anova(m10)
vif(m10,type="predictor")
plot(allEffects(m10), main = "Model m10")
par(mfrow=c(2,2))
plot(m10, id.n=0, main = "Model m10")
par(mfrow=c(1,1))
marginalModelPlots(m10, main = "Model m10")
bptest(m10)
avPlots(m10, id=list(method=list(cooks.distance(m10),"x"), n = 5),
        main = "Model m10")
residualPlots(m10, main = "Model m10")
par(mfrow=c(1,1))
influencePlot(m10,main = "Model m10")

```

### Quality Info Model m11

```{r qualitym11}
summary(m11)
anova(m11)
vif(m11,type="predictor")
plot(allEffects(m11), main = "Model m11")
par(mfrow=c(2,2))
plot(m11, id.n=0, main = "Model m11")
par(mfrow=c(1,1))
marginalModelPlots(m11, main = "Model m11")
bptest(m11)
avPlots(m11, id=list(method=list(cooks.distance(m11),"x"), n = 5),
        main = "Model m11")
residualPlots(m11, main = "Model m11")
par(mfrow=c(1,1))
influencePlot(m11,main = "Model m11")
```
